{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQL29+ullWtjiFDDeVfYLi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParitoshP702/Bilevel-Optimization/blob/main/Grid_Search(CIFAR).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp_57BteSiGQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gurobipy"
      ],
      "metadata": {
        "id": "jn37nFVBTOAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gurobipy as gp"
      ],
      "metadata": {
        "id": "o3DxLLZwTbWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " params = {\n",
        "  \"WLSACCESSID\": '753e7886-7142-449d-8baa-d41ca78716ef',\n",
        "  \"WLSSECRET\": '880d2525-364b-41d0-ac23-6dcf7ad15312',\n",
        "  \"LICENSEID\": 914249,\n",
        "  }\n",
        "env = gp.Env(params=params)"
      ],
      "metadata": {
        "id": "ZlAYDUOPTdNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,Y_train),(X_test,Y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "tUe7OqSwSrAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_count = 3500\n",
        "eval_count = 1000\n",
        "test_count = 1000"
      ],
      "metadata": {
        "id": "C8PWgutLStCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255.0\n",
        "X_test  = X_test/255.0"
      ],
      "metadata": {
        "id": "piBGKtEiSu-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = X_train[:train_count,:,:,:]\n",
        "x_eval = X_train[train_count:train_count+eval_count,:,:,:]\n",
        "y_train = Y_train[:train_count]\n",
        "y_eval = Y_train[train_count:train_count+eval_count]"
      ],
      "metadata": {
        "id": "Ppv357ueSw_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = X_test[:test_count,:,:,:]\n",
        "y_test = Y_test[:test_count]"
      ],
      "metadata": {
        "id": "FkbQ4ADMSy3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flattening our datasets\n",
        "x_train = x_train.reshape(x_train.shape[0],-1)\n",
        "x_eval = x_eval.reshape(x_eval.shape[0],-1)\n",
        "x_test = x_test.reshape(x_test.shape[0],-1)"
      ],
      "metadata": {
        "id": "wxAycsIPS1cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_training = np.zeros(shape = (len(y_train),10), dtype = float)#one hot encoding the training labels\n",
        "for i in range(len(y_train)):\n",
        "  for j in range(10):\n",
        "    if j  == y_train[i]:\n",
        "      y_training[i][j] = 1.0\n",
        "    else:\n",
        "      y_training[i][j] = 0.0"
      ],
      "metadata": {
        "id": "2TG8PrtvS3Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_array = np.zeros(shape = (len(y_eval),10),dtype  =float)#one hot encoding the validation labels\n",
        "for i in range(len(y_eval)):\n",
        "  for j in range(10):\n",
        "    if j  == y_eval[i]:\n",
        "      y_val_array[i][j] = 1.0\n",
        "    else:\n",
        "      y_val_array[i][j] = 0.0"
      ],
      "metadata": {
        "id": "0IMvdpp3S52W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "GkSqizs3S7tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_weight_array(model):\n",
        "  weights_list = []\n",
        "  for i in range(len(model.weights)):\n",
        "    weights_array = tf.make_ndarray(tf.make_tensor_proto(model.weights[i]))\n",
        "    if i%2 == 0:\n",
        "      shape_array = weights_array.shape\n",
        "      for j in range(shape_array[0]):\n",
        "        for k in range(shape_array[1]):\n",
        "          weights_list.append(weights_array[j][k])\n",
        "          # if len(weights_list_new) < skip_length:\n",
        "          #   weights_list_new.append(0)\n",
        "          # else:\n",
        "          #   weights_list_new.append(weights_array[j][k])\n",
        "    else:\n",
        "      lgt = weights_array.shape[0]\n",
        "      for j in range(lgt):\n",
        "        weights_list.append(weights_array[j])\n",
        "        # if len(weights_list_new) < skip_length:\n",
        "        #   weights_list_new.append(0)\n",
        "        # else:\n",
        "        #   weights_list_new.append(weights_array[j])\n",
        "  return np.array(weights_list)"
      ],
      "metadata": {
        "id": "nlLboNFnS9xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_array_for_hessian(model):\n",
        "  skip_length = len(model.layers[0].weights[0].numpy().reshape(-1)) + len(model.layers[0].weights[1].numpy().reshape(-1))\n",
        "  weights_list = []\n",
        "  for i in range(len(model.weights)):\n",
        "    weights_array = tf.make_ndarray(tf.make_tensor_proto(model.weights[i]))\n",
        "    if i%2 == 0:\n",
        "      shape_array = weights_array.shape\n",
        "      for j in range(shape_array[0]):\n",
        "        for k in range(shape_array[1]):\n",
        "          # weights_list.append(weights_array[j][k])\n",
        "          if len(weights_list) < skip_length:\n",
        "            weights_list.append(0)\n",
        "          else:\n",
        "            weights_list.append(weights_array[j][k])\n",
        "    else:\n",
        "      lgt = weights_array.shape[0]\n",
        "      for j in range(lgt):\n",
        "        # weights_list.append(weights_array[j])\n",
        "        if len(weights_list) < skip_length:\n",
        "          weights_list.append(0)\n",
        "        else:\n",
        "          weights_list.append(weights_array[j])\n",
        "  return np.array(weights_list)"
      ],
      "metadata": {
        "id": "5CF2VmSiS_54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(x_target,y_target,model):##general function which returns the list of gradient vector as an numpy array\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_object = tf.keras.losses.MeanSquaredError()\n",
        "    y_pred_array = model(x_target,training = True)\n",
        "    loss = loss_object(y_target,y_pred_array)\n",
        "  g = tape.gradient(loss,model.trainable_variables)\n",
        "  final_grad_list = []\n",
        "  for i in range(len(g)):\n",
        "    grad_array = tf.make_ndarray(tf.make_tensor_proto(g[i]))\n",
        "    if i%2==0:\n",
        "      grad_shape = grad_array.shape\n",
        "      for j in range(grad_shape[0]):\n",
        "        for k in range(grad_shape[1]):\n",
        "          final_grad_list.append(grad_array[j][k])\n",
        "    else:\n",
        "      length = grad_array.shape[0]\n",
        "      for j in range(length):\n",
        "        final_grad_list.append(grad_array[j])\n",
        "  return np.array(final_grad_list)\n"
      ],
      "metadata": {
        "id": "ZcMx4ygcTB8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hessian(model):\n",
        "  final_hessian_list = []\n",
        "  with tf.GradientTape(persistent = True) as tape1:\n",
        "    with tf.GradientTape(persistent = True) as tape2:\n",
        "      loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "      y_pred_array = model(x_train,training = True)\n",
        "      loss = loss_object(y_training,y_pred_array)\n",
        "    g = tape2.gradient(loss, model.trainable_variables)\n",
        "  for i in range(len(g)):\n",
        "    # reshaped_grad = tf.reshape(g[i], [-1])\n",
        "    h = tape1.jacobian(g[i],model.trainable_variables)\n",
        "    final_hessian_list.append(h)\n",
        "\n",
        "\n",
        "  ##Now this final hessian list is actually a double dimensional list of tensors, so we will convert it into a matrix\n",
        "  #reshaping the double dimensional list of tensors into a matrix\n",
        "  hessian_matrix = np.empty(shape = (1,1),dtype = float)\n",
        "  for i in range(len(final_hessian_list)):\n",
        "    hess_col_mat = np.empty(shape = (1,1),dtype = float)\n",
        "    for j in range(len(final_hessian_list[i])):\n",
        "      hess_array = tf.make_ndarray(tf.make_tensor_proto(final_hessian_list[i][j]))\n",
        "      hess_shape = hess_array.shape\n",
        "      if i%2 == 0:\n",
        "        if j%2 == 0:\n",
        "          hess_array = hess_array.reshape(hess_shape[0]*hess_shape[1],hess_shape[2]*hess_shape[3])\n",
        "        else:\n",
        "          hess_array = hess_array.reshape(hess_shape[0]*hess_shape[1],hess_shape[2])\n",
        "      else:\n",
        "        if j%2 == 0:\n",
        "          hess_array = hess_array.reshape(hess_shape[0],hess_shape[1]*hess_shape[2])\n",
        "        else:\n",
        "          hess_array = hess_array\n",
        "      if j==0 :\n",
        "        hess_col_mat = hess_array\n",
        "      else:\n",
        "        hess_col_mat = np.concatenate((hess_col_mat,hess_array),axis = 1)\n",
        "    if i==0:\n",
        "      hessian_matrix = hess_col_mat\n",
        "    else:\n",
        "      hessian_matrix= np.concatenate((hessian_matrix,hess_col_mat),axis = 0)\n",
        "\n",
        "\n",
        "  return hessian_matrix\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "9A51d5_RTEAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_fine_tuning(model,params_model):\n",
        "  number_of_layers = params_model[1]\n",
        "  reg_param = params_model[3]\n",
        "  neurons_per_layer = params_model[0]\n",
        "  activation_fun = params_model[2]\n",
        "\n",
        "  ###calculating the hessian for the model and the gradient of the validation loss\n",
        "  hessian_matrix = compute_hessian(model)\n",
        "  grad_validation = compute_gradient(x_eval,y_val_array,model)\n",
        "  final_weights_array_new = weight_array_for_hessian(model)\n",
        "  l = len(final_weights_array_new)\n",
        "\n",
        "\n",
        "  ##adding the regularization term in the hessian\n",
        "  weight_array_vec = final_weights_array_new.reshape(l,1)/len(y_train)\n",
        "  hessian_col_mat = np.concatenate((weight_array_vec,hessian_matrix),axis = 1)\n",
        "  weight_array_withreg = np.concatenate(([[0]],final_weights_array_new.reshape(1,l)),axis = 1)/len(y_train)\n",
        "  hessian_mat_with_reg = np.concatenate((weight_array_withreg,hessian_col_mat),axis = 0)\n",
        "\n",
        "\n",
        "  grad_validation_new = np.concatenate(([[0]],grad_validation.reshape(1,l)),axis = 1)#validation array with regularization\n",
        "\n",
        "\n",
        "  ##Solving the linear program\n",
        "  ub = [10 for i in range(l+1)]\n",
        "  lb = []\n",
        "  for i in range(l+1):\n",
        "    if i==0:\n",
        "      lb.append(1e-5)\n",
        "    else:\n",
        "      lb.append(-10)\n",
        "\n",
        "\n",
        "  # Create the model within the Gurobi environment\n",
        "  m = gp.Model(env=env)\n",
        "  # m = gp.Model()\n",
        "  x = m.addMVar((l+1,),lb = lb, ub = ub )\n",
        "  m.setObjective(grad_validation_new@x)\n",
        "  # m.addConstr(hessian_mat_with_reg@x == 0)\n",
        "  m.addConstr([hessian_mat_with_reg[1:]@x <= 0.1)\n",
        "  m.addConstr(hessian_mat_with_reg@[1:]x >= -0.1)\n",
        "  x.PStart = np.zeros(l+1)\n",
        "  # GRBModel.Set(Pstart = np.zeros(l+1))\n",
        "  m.optimize()\n",
        "  all_vars = m.getVars()\n",
        "  values = m.getAttr(\"x\",all_vars)\n",
        "  values = np.array(values)\n",
        "  values = values/np.linalg.norm(values)\n",
        "\n",
        "  final_weights_array = complete_weight_array(model)\n",
        "  weight_array_with_reg = np.concatenate(([[reg_param]],final_weights_array.reshape(1,l)),axis = 1).reshape(-1)\n",
        "  descent_factors = []\n",
        "  for i in range(-100,20000,200):\n",
        "    descent_factors.append(i*1e-3)\n",
        "  descent_factors = np.array(descent_factors)\n",
        "\n",
        "\n",
        "  weight_sample_space_matrix = np.empty(shape = (len(descent_factors),len(weight_array_with_reg)),dtype = float)##initializing the weight sample space matrix\n",
        "  for i in range(len(descent_factors)):\n",
        "    weight_sample_space_matrix[i] =weight_array_with_reg+ values*descent_factors[i]   ##assigning values to the weight sample space matrix\n",
        "\n",
        "\n",
        "  ##defining the loss object\n",
        "  loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "  ##computation for validation loss\n",
        "  def validation_loss_computation(weight_and_reg_array):##function which computes the loss score of the model corresponding to given weights\n",
        "\n",
        "      model_demo = Sequential()\n",
        "      model_demo.add(Dense(units = 2, input_dim = 3072))\n",
        "      for i in range(number_of_layers):\n",
        "          model_demo.add(Dense(units = neurons_per_layer, activation = activation_fun, kernel_regularizer = tf.keras.regularizers.L2(weight_and_reg_array[0])))\n",
        "      model_demo.add(Dense(units = 10,activation = \"softmax\", kernel_regularizer = tf.keras.regularizers.L2(weight_and_reg_array[0])))\n",
        "      model_demo.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = \"Adam\", metrics = [\"accuracy\"])\n",
        "      weight_tracker = 1##as \"weight_and_reg_array\" is a one dimensional array it keeps track of the indices of the array\n",
        "      for i in range(len(model_demo.layers)):##changing the weights of the model layer wise\n",
        "        orignal_weight_list = model.layers[i].weights\n",
        "        array_1 = orignal_weight_list[0].numpy()##array corresponding to the weight matrix\n",
        "        array_2 = orignal_weight_list[1].numpy()##array corresponding to the bias vector\n",
        "        array_1_new = weight_and_reg_array[weight_tracker:weight_tracker+array_1.shape[0]*array_1.shape[1]]\n",
        "        weight_tracker += array_1.shape[0]*array_1.shape[1]##updating the weight tracker\n",
        "        array_2_new = weight_and_reg_array[weight_tracker:weight_tracker + array_2.shape[0]]\n",
        "        weight_tracker += array_2.shape[0] #updating the weight tracker\n",
        "        array_1_new = array_1_new.reshape(array_1.shape) ##new weight matrix\n",
        "        array_2_new = array_2_new.reshape(array_2.shape) ##new bias vector\n",
        "        list_of_new_array = [] ##list of the new weight matrix and the new bias vector\n",
        "        list_of_new_array.append(array_1_new)\n",
        "        list_of_new_array.append(array_2_new)\n",
        "        model_demo.layers[i].set_weights(list_of_new_array) ##appending the new weights into the given layer of the model\n",
        "      y_pred_array = model_demo(np.array(x_eval),training = False)\n",
        "      y_pred_training = model_demo(np.array(x_train),training = False)\n",
        "      loss = loss_object(y_val_array,y_pred_array)\n",
        "      loss_t = loss_object(y_training,y_pred_training)\n",
        "      # loss1,_ = model_demo.evaluate(x_eval,y_eval,verbose= 0)\n",
        "      # loss2,_= model_demo.evaluate(x_train,y_train,verbose = 0)\n",
        "      return loss,loss_t,model_demo\n",
        "\n",
        "\n",
        "  loss_array_valid = np.empty(shape = len(descent_factors),dtype = float)##array to contain the training losses\n",
        "  loss_array_train = np.empty(shape = len(descent_factors),dtype = float)##array to contain the validation losses\n",
        "\n",
        "  for i in range(len(loss_array_valid)):\n",
        "    loss_array_valid[i] ,loss_array_train[i],_= validation_loss_computation(weight_sample_space_matrix[i])\n",
        "\n",
        "\n",
        "  ideal_weight_array = weight_sample_space_matrix[loss_array_valid.argmin()]\n",
        "  ideal_regularization_parameter = ideal_weight_array[0]\n",
        "  _,_,best_model = validation_loss_computation(ideal_weight_array)\n",
        "\n",
        "  return ideal_weight_array[0],best_model,loss_array_valid.min()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "7e4-DuRpTGUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_population():\n",
        "  neurons_per_layer = [5,10,15]\n",
        "  number_of_layers = [1,2,3]\n",
        "  activation_function = [\"relu\",\"sigmoid\",\"tanh\"]\n",
        "  regularization_param = [1e-10,1e-9,1e-8]\n",
        "  param_dict = []\n",
        "  for i in range(len(neurons_per_layer)):\n",
        "    for j in range(len(number_of_layers)):\n",
        "      for k in range(len(activation_function)):\n",
        "        for l in range(len(regularization_param)):\n",
        "          dic = [neurons_per_layer[i],number_of_layers[j],activation_function[k],regularization_param[l]]\n",
        "          param_dict.append(dic)\n",
        "  return param_dict\n"
      ],
      "metadata": {
        "id": "2o6Mol3YTIoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(parameters,initialWeights=None):\n",
        "    neurons_per_layer = parameters[0]\n",
        "    no_of_layers = parameters[1]\n",
        "    activation_function = parameters[2]\n",
        "\n",
        "    #Following is not used here\n",
        "    # optimization_method = parameters[3]\n",
        "    regularization_param = parameters[3]\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=2, input_dim=3072))\n",
        "\n",
        "    for _ in range(no_of_layers):\n",
        "        model.add(Dense(units=neurons_per_layer, activation=activation_function,kernel_regularizer = tf.keras.regularizers.L2(regularization_param)))\n",
        "\n",
        "    model.add(Dense(units = 10,  activation = 'softmax',kernel_regularizer = tf.keras.regularizers.L2(regularization_param)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return(model)\n",
        "\n",
        "def evaluate_model(individual,initialWeights=None):\n",
        "    model = train_model(individual,initialWeights)\n",
        "\n",
        "    #The last element in the individual should always be the optimizer\n",
        "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"Adam\", metrics=['accuracy'])\n",
        "    model.fit(x_train, y_training, batch_size = 64, epochs = 10)\n",
        "\n",
        "    # print(\"Training Accuracy:\", model.evaluate(x_train, y_train, verbose = 0)[1])\n",
        "\n",
        "    #Evaluate on evaluation data\n",
        "    # loss_score, accuracy_score = model.evaluate(x_eval, y_val_array, verbose = 0)\n",
        "    y_pred = model(x_eval,training = False)\n",
        "    loss_score = loss_object(y_pred,y_val_array)\n",
        "    return loss_score,model"
      ],
      "metadata": {
        "id": "gCTNlcc0TkAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_population = generate_population()\n",
        "losses = []\n",
        "models = []\n",
        "# losses = [evaluate_model(individual) for individual in initial_population]\n",
        "for individual in initial_population:\n",
        "  loss,model = evaluate_model(individual)\n",
        "  losses.append(loss)\n",
        "  models.append(model)\n",
        "losses_new = np.zeros(len(losses))\n",
        "models_new = [None]*(len(models))\n",
        "for i in range(len(models)):\n",
        "  initial_population[i][3],models_new[i],losses_new[i] = perform_fine_tuning(models[i],initial_population[i])"
      ],
      "metadata": {
        "id": "7T8X87O-Tnsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(losses).min()"
      ],
      "metadata": {
        "id": "nVvflrE5Tqxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_new.min()"
      ],
      "metadata": {
        "id": "SnMAQ9cYT37j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_param = initial_population[np.array(losses).argmin()]"
      ],
      "metadata": {
        "id": "dKidDId1T6dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = best_param[3]\n",
        "number_of_layers=  best_param[0]\n",
        "neuron_per_layer = best_param[1]\n",
        "activation_function = best_param[2]\n"
      ],
      "metadata": {
        "id": "pNspjvS7T8kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential()\n",
        "# model.add(Dense(units = 2,input_dim = 3072 ))\n",
        "# for i in range(number_of_layers):\n",
        "#   model.add(Dense(units = neuron_per_layer, activation = activation_function, kernel_regularizer = tf.keras.regularizers.L2(reg)))\n",
        "# model.add(Dense(units = 10,activation = \"softmax\", kernel_regularizer = tf.keras.regularizers.L2(reg) ))"
      ],
      "metadata": {
        "id": "YCK3qVwlT_So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = \"Adam\", metrics = [\"accuracy\"])\n",
        "# model.fit(x_train,y_training, epochs = 10,batch_size = 64)\n",
        "# y_pred = model(x_eval,training = False)\n",
        "# loss_object(y_pred,y_val_array)"
      ],
      "metadata": {
        "id": "nbraYd2AUBpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = models_new[np.array(losses_new).argmin()]"
      ],
      "metadata": {
        "id": "DF0RVCxnUc5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,accuracy  = best_model.evaluate(x_eval,y_val_array,verbose = 0)"
      ],
      "metadata": {
        "id": "fHsMZ1kzLTly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy##validation accuracy"
      ],
      "metadata": {
        "id": "XvlxRa7NMEDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_val = best_model(x_eval,training = False)\n"
      ],
      "metadata": {
        "id": "MqAMSmu6Lb2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object(y_pred_val,y_val_array).numpy()##validation loss"
      ],
      "metadata": {
        "id": "gtJK_-w5Lixu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,accuracy_test = best_model.fit(x_test,y_testing,verbose = 0)"
      ],
      "metadata": {
        "id": "4PSW1slBLr95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_test"
      ],
      "metadata": {
        "id": "VONssITZLyWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = best_model(x_test,training = False)"
      ],
      "metadata": {
        "id": "knCQOtu0L0hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object(y_testing,y_pred_test).numpy()##test loss"
      ],
      "metadata": {
        "id": "6BKS7RY3L6-f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}