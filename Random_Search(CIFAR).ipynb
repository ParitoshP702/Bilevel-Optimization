{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParitoshP702/Bilevel-Optimization/blob/main/Random_Search(CIFAR).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAags-AWjlrW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gurobipy"
      ],
      "metadata": {
        "id": "a5AcwkNqTQ8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b671760f-98af-4db4-eec4-ae4877310400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gurobipy\n",
            "  Downloading gurobipy-10.0.0-cp38-cp38-manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gurobipy\n",
            "Successfully installed gurobipy-10.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gurobipy as gp"
      ],
      "metadata": {
        "id": "aoESsDGoTTHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " params = {\n",
        "  \"WLSACCESSID\": '753e7886-7142-449d-8baa-d41ca78716ef',\n",
        "  \"WLSSECRET\": '880d2525-364b-41d0-ac23-6dcf7ad15312',\n",
        "  \"LICENSEID\": 914249,\n",
        "  }\n",
        "env = gp.Env(params=params)"
      ],
      "metadata": {
        "id": "g_O4iwEBTVGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be09902e-20d3-478e-d061-5f934c493afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set parameter WLSAccessID\n",
            "Set parameter WLSSecret\n",
            "Set parameter LicenseID to value 914249\n",
            "Academic license - for non-commercial use only - registered to ppankaj21@iitk.ac.in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,Y_train),(X_test,Y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "Dm08Y9-zjxtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62159b2d-0f54-445c-eaa6-4fe80621ff60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 14s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_count = 2000\n",
        "eval_count = 1000\n",
        "test_count = 1000"
      ],
      "metadata": {
        "id": "0uBd6Gzlj2RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255.0\n",
        "X_test  = X_test/255.0"
      ],
      "metadata": {
        "id": "4E0e6tiYj4zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = X_train[:train_count,:,:,:]\n",
        "x_eval = X_train[train_count:train_count+eval_count,:,:,:]\n",
        "y_train = Y_train[:train_count]\n",
        "y_eval = Y_train[train_count:train_count+eval_count]"
      ],
      "metadata": {
        "id": "fmTx7Vsoj6oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = X_test[:test_count,:,:,:]\n",
        "y_test = Y_test[:test_count]"
      ],
      "metadata": {
        "id": "mG-yEW4-j9Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flattening our datasets\n",
        "x_train = x_train.reshape(x_train.shape[0],-1)\n",
        "x_eval = x_eval.reshape(x_eval.shape[0],-1)\n",
        "x_test = x_test.reshape(x_test.shape[0],-1)"
      ],
      "metadata": {
        "id": "n5zznDy1j_mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_training = np.zeros(shape = (len(y_train),10), dtype = float)#one hot encoding the training labels\n",
        "for i in range(len(y_train)):\n",
        "  for j in range(10):\n",
        "    if j  == y_train[i]:\n",
        "      y_training[i][j] = 1.0\n",
        "    else:\n",
        "      y_training[i][j] = 0.0"
      ],
      "metadata": {
        "id": "cLVB3awNkBaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_array = np.zeros(shape = (len(y_eval),10),dtype  =float)#one hot encoding the validation labels\n",
        "for i in range(len(y_eval)):\n",
        "  for j in range(10):\n",
        "    if j  == y_eval[i]:\n",
        "      y_val_array[i][j] = 1.0\n",
        "    else:\n",
        "      y_val_array[i][j] = 0.0"
      ],
      "metadata": {
        "id": "MHFRd99ikEAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_testing = np.zeros(shape = (len(y_test),10),dtype  =float)#one hot encoding the validation labels\n",
        "for i in range(len(y_test)):\n",
        "  for j in range(10):\n",
        "    if j  == y_test[i]:\n",
        "      y_testing[i][j] = 1.0\n",
        "    else:\n",
        "      y_testing[i][j] = 0.0"
      ],
      "metadata": {
        "id": "b_zPd5mAVY5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "UiqBGjiukGVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_weight_array(model):\n",
        "  weights_list = []\n",
        "  for i in range(len(model.weights)):\n",
        "    weights_array = tf.make_ndarray(tf.make_tensor_proto(model.weights[i]))\n",
        "    if i%2 == 0:\n",
        "      shape_array = weights_array.shape\n",
        "      for j in range(shape_array[0]):\n",
        "        for k in range(shape_array[1]):\n",
        "          weights_list.append(weights_array[j][k])\n",
        "          # if len(weights_list_new) < skip_length:\n",
        "          #   weights_list_new.append(0)\n",
        "          # else:\n",
        "          #   weights_list_new.append(weights_array[j][k])\n",
        "    else:\n",
        "      lgt = weights_array.shape[0]\n",
        "      for j in range(lgt):\n",
        "        weights_list.append(weights_array[j])\n",
        "        # if len(weights_list_new) < skip_length:\n",
        "        #   weights_list_new.append(0)\n",
        "        # else:\n",
        "        #   weights_list_new.append(weights_array[j])\n",
        "  return np.array(weights_list)"
      ],
      "metadata": {
        "id": "kTEK8adfkI_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_array_for_hessian(model):\n",
        "  skip_length = len(model.layers[0].weights[0].numpy().reshape(-1)) + len(model.layers[0].weights[1].numpy().reshape(-1))\n",
        "  weights_list = []\n",
        "  for i in range(len(model.weights)):\n",
        "    weights_array = tf.make_ndarray(tf.make_tensor_proto(model.weights[i]))\n",
        "    if i%2 == 0:\n",
        "      shape_array = weights_array.shape\n",
        "      for j in range(shape_array[0]):\n",
        "        for k in range(shape_array[1]):\n",
        "          # weights_list.append(weights_array[j][k])\n",
        "          if len(weights_list) < skip_length:\n",
        "            weights_list.append(0)\n",
        "          else:\n",
        "            weights_list.append(weights_array[j][k])\n",
        "    else:\n",
        "      lgt = weights_array.shape[0]\n",
        "      for j in range(lgt):\n",
        "        # weights_list.append(weights_array[j])\n",
        "        if len(weights_list) < skip_length:\n",
        "          weights_list.append(0)\n",
        "        else:\n",
        "          weights_list.append(weights_array[j])\n",
        "  return np.array(weights_list)"
      ],
      "metadata": {
        "id": "7D2BMjfwkKyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(x_target,y_target,model):##general function which returns the list of gradient vector as an numpy array\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_object = tf.keras.losses.MeanSquaredError()\n",
        "    y_pred_array = model(x_target,training = True)\n",
        "    loss = loss_object(y_target,y_pred_array)\n",
        "  g = tape.gradient(loss,model.trainable_variables)\n",
        "  final_grad_list = []\n",
        "  for i in range(len(g)):\n",
        "    grad_array = tf.make_ndarray(tf.make_tensor_proto(g[i]))\n",
        "    if i%2==0:\n",
        "      grad_shape = grad_array.shape\n",
        "      for j in range(grad_shape[0]):\n",
        "        for k in range(grad_shape[1]):\n",
        "          final_grad_list.append(grad_array[j][k])\n",
        "    else:\n",
        "      length = grad_array.shape[0]\n",
        "      for j in range(length):\n",
        "        final_grad_list.append(grad_array[j])\n",
        "  return np.array(final_grad_list)\n"
      ],
      "metadata": {
        "id": "3swD0wx1kNpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hessian(model):\n",
        "  final_hessian_list = []\n",
        "  with tf.GradientTape(persistent = True) as tape1:\n",
        "    with tf.GradientTape(persistent = True) as tape2:\n",
        "      loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "      y_pred_array = model(x_train,training = True)\n",
        "      loss = loss_object(y_training,y_pred_array)\n",
        "    g = tape2.gradient(loss, model.trainable_variables)\n",
        "  for i in range(len(g)):\n",
        "    # reshaped_grad = tf.reshape(g[i], [-1])\n",
        "    h = tape1.jacobian(g[i],model.trainable_variables)\n",
        "    final_hessian_list.append(h)\n",
        "\n",
        "\n",
        "  ##Now this final hessian list is actually a double dimensional list of tensors, so we will convert it into a matrix\n",
        "  #reshaping the double dimensional list of tensors into a matrix\n",
        "  hessian_matrix = np.empty(shape = (1,1),dtype = float)\n",
        "  for i in range(len(final_hessian_list)):\n",
        "    hess_col_mat = np.empty(shape = (1,1),dtype = float)\n",
        "    for j in range(len(final_hessian_list[i])):\n",
        "      hess_array = tf.make_ndarray(tf.make_tensor_proto(final_hessian_list[i][j]))\n",
        "      hess_shape = hess_array.shape\n",
        "      if i%2 == 0:\n",
        "        if j%2 == 0:\n",
        "          hess_array = hess_array.reshape(hess_shape[0]*hess_shape[1],hess_shape[2]*hess_shape[3])\n",
        "        else:\n",
        "          hess_array = hess_array.reshape(hess_shape[0]*hess_shape[1],hess_shape[2])\n",
        "      else:\n",
        "        if j%2 == 0:\n",
        "          hess_array = hess_array.reshape(hess_shape[0],hess_shape[1]*hess_shape[2])\n",
        "        else:\n",
        "          hess_array = hess_array\n",
        "      if j==0 :\n",
        "        hess_col_mat = hess_array\n",
        "      else:\n",
        "        hess_col_mat = np.concatenate((hess_col_mat,hess_array),axis = 1)\n",
        "    if i==0:\n",
        "      hessian_matrix = hess_col_mat\n",
        "    else:\n",
        "      hessian_matrix= np.concatenate((hessian_matrix,hess_col_mat),axis = 0)\n",
        "\n",
        "\n",
        "  return hessian_matrix\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "V71816XNkQ5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_fine_tuning(model,params_model):\n",
        "  number_of_layers = params_model[1]\n",
        "  reg_param = params_model[3]\n",
        "  neurons_per_layer = params_model[0]\n",
        "  activation_fun = params_model[2]\n",
        "\n",
        "  ###calculating the hessian for the model and the gradient of the validation loss\n",
        "  hessian_matrix = compute_hessian(model)\n",
        "  grad_validation = compute_gradient(x_eval,y_val_array,model)\n",
        "  final_weights_array_new = weight_array_for_hessian(model)\n",
        "  l = len(final_weights_array_new)\n",
        "\n",
        "\n",
        "  ##adding the regularization term in the hessian\n",
        "  weight_array_vec = final_weights_array_new.reshape(l,1)/len(y_train)\n",
        "  hessian_col_mat = np.concatenate((weight_array_vec,hessian_matrix),axis = 1)\n",
        "  weight_array_withreg = np.concatenate(([[0]],final_weights_array_new.reshape(1,l)),axis = 1)/len(y_train)\n",
        "  hessian_mat_with_reg = np.concatenate((weight_array_withreg,hessian_col_mat),axis = 0)\n",
        "\n",
        "\n",
        "  grad_validation_new = np.concatenate(([[0]],grad_validation.reshape(1,l)),axis = 1)#validation array with regularization\n",
        "\n",
        "\n",
        "  ##Solving the linear program\n",
        "  ub = [10 for i in range(l+1)]\n",
        "  lb = []\n",
        "  for i in range(l+1):\n",
        "    if i==0:\n",
        "      lb.append(1e-5)\n",
        "    else:\n",
        "      lb.append(-10)\n",
        "\n",
        "\n",
        "  # Create the model within the Gurobi environment\n",
        "  m = gp.Model(env=env)\n",
        "  # m = gp.Model()\n",
        "  x = m.addMVar((l+1,),lb = lb, ub = ub )\n",
        "  m.setObjective(grad_validation_new@x)\n",
        "  # m.addConstr(hessian_mat_with_reg@x == 0)\n",
        "  m.addConstr(hessian_mat_with_reg@x <= 0.1)\n",
        "  m.addConstr(hessian_mat_with_reg@x >= -0.1)\n",
        "  x.PStart = np.zeros(l+1)\n",
        "  # GRBModel.Set(Pstart = np.zeros(l+1))\n",
        "  m.optimize()\n",
        "  all_vars = m.getVars()\n",
        "  values = m.getAttr(\"x\",all_vars)\n",
        "  values = np.array(values)\n",
        "  values = values/np.linalg.norm(values)\n",
        "\n",
        "  final_weights_array = complete_weight_array(model)\n",
        "  weight_array_with_reg = np.concatenate(([[reg_param]],final_weights_array.reshape(1,l)),axis = 1).reshape(-1)\n",
        "  descent_factors = []\n",
        "  for i in range(-100,20000,200):\n",
        "    descent_factors.append(i*1e-3)\n",
        "  descent_factors = np.array(descent_factors)\n",
        "\n",
        "\n",
        "  weight_sample_space_matrix = np.empty(shape = (len(descent_factors),len(weight_array_with_reg)),dtype = float)##initializing the weight sample space matrix\n",
        "  for i in range(len(descent_factors)):\n",
        "    weight_sample_space_matrix[i] =weight_array_with_reg+ values*descent_factors[i]   ##assigning values to the weight sample space matrix\n",
        "\n",
        "\n",
        "  ##defining the loss object\n",
        "  loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "  ##computation for validation loss\n",
        "  def validation_loss_computation(weight_and_reg_array):##function which computes the loss score of the model corresponding to given weights\n",
        "\n",
        "      model_demo = Sequential()\n",
        "      model_demo.add(Dense(units = 2, input_dim = 3072))\n",
        "      for i in range(number_of_layers):\n",
        "          model_demo.add(Dense(units = neurons_per_layer, activation = activation_fun, kernel_regularizer = tf.keras.regularizers.L2(weight_and_reg_array[0])))\n",
        "      model_demo.add(Dense(units = 10,activation = \"softmax\", kernel_regularizer = tf.keras.regularizers.L2(weight_and_reg_array[0])))\n",
        "      model_demo.compile(loss = \"mean_squared_error\", optimizer = \"Adam\", metrics = [\"accuracy\"])\n",
        "      weight_tracker = 1##as \"weight_and_reg_array\" is a one dimensional array it keeps track of the indices of the array\n",
        "      for i in range(len(model_demo.layers)):##changing the weights of the model layer wise\n",
        "        orignal_weight_list = model.layers[i].weights\n",
        "        array_1 = orignal_weight_list[0].numpy()##array corresponding to the weight matrix\n",
        "        array_2 = orignal_weight_list[1].numpy()##array corresponding to the bias vector\n",
        "        array_1_new = weight_and_reg_array[weight_tracker:weight_tracker+array_1.shape[0]*array_1.shape[1]]\n",
        "        weight_tracker += array_1.shape[0]*array_1.shape[1]##updating the weight tracker\n",
        "        array_2_new = weight_and_reg_array[weight_tracker:weight_tracker + array_2.shape[0]]\n",
        "        weight_tracker += array_2.shape[0] #updating the weight tracker\n",
        "        array_1_new = array_1_new.reshape(array_1.shape) ##new weight matrix\n",
        "        array_2_new = array_2_new.reshape(array_2.shape) ##new bias vector\n",
        "        list_of_new_array = [] ##list of the new weight matrix and the new bias vector\n",
        "        list_of_new_array.append(array_1_new)\n",
        "        list_of_new_array.append(array_2_new)\n",
        "        model_demo.layers[i].set_weights(list_of_new_array) ##appending the new weights into the given layer of the model\n",
        "      y_pred_array = model_demo(np.array(x_eval),training = False)\n",
        "      y_pred_training = model_demo(np.array(x_train),training = False)\n",
        "      loss = loss_object(y_val_array,y_pred_array)\n",
        "      loss_t = loss_object(y_training,y_pred_training)\n",
        "      # loss1,_ = model_demo.evaluate(x_eval,y_eval,verbose= 0)\n",
        "      # loss2,_= model_demo.evaluate(x_train,y_train,verbose = 0)\n",
        "      return loss,loss_t,model_demo\n",
        "\n",
        "\n",
        "  loss_array_valid = np.empty(shape = len(descent_factors),dtype = float)##array to contain the training losses\n",
        "  loss_array_train = np.empty(shape = len(descent_factors),dtype = float)##array to contain the validation losses\n",
        "\n",
        "  for i in range(len(loss_array_valid)):\n",
        "    loss_array_valid[i] ,loss_array_train[i],_= validation_loss_computation(weight_sample_space_matrix[i])\n",
        "\n",
        "\n",
        "  ideal_weight_array = weight_sample_space_matrix[loss_array_valid.argmin()]\n",
        "  ideal_regularization_parameter = ideal_weight_array[0]\n",
        "  _,_,best_model = validation_loss_computation(ideal_weight_array)\n",
        "\n",
        "  return ideal_weight_array[0],best_model,loss_array_valid.min()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "gAZ6XwBxkT-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparameters(returnAs='vals'):\n",
        "    parameters = {}\n",
        "\n",
        "    #Add other parameters here\n",
        "\n",
        "    parameters[\"neurons_per_layer\"] = [5,10,15]\n",
        "    parameters[\"number_of_layers\"] = [1, 2, 3]\n",
        "    parameters[\"activation_function\"] = ['relu', 'tanh', 'sigmoid']\n",
        "    parameters[\"regularization_parameter\"] = [1e-10,1e-9,1e-8]\n",
        "    #Search over regularization parameter as well\n",
        "    #parameters[\"regularization\"] = []\n",
        "\n",
        "    #Keep the last one as optimizer\n",
        "    #parameters[\"optimization_method\"] = ['adam', 'rmsprop']\n",
        "    # parameters[\"optimization_method\"] = ['adam']\n",
        "\n",
        "    if returnAs == 'dict': return(parameters)\n",
        "    if returnAs == 'vals': return(list(parameters.values()))\n",
        "    if returnAs == 'keys': return(list(parameters.keys()))\n",
        "\n",
        "# def hyperparameters_old():\n",
        "#     parameters = []\n",
        "#     units_per_layer = [5, 10, 15]\n",
        "#     layers = [1, 2, 3]\n",
        "#     activation = ['relu', 'tanh', 'sigmoid']\n",
        "#     optimizer = ['adam', 'rmsprop']\n",
        "#     parameters.append(units_per_layer)\n",
        "#     parameters.append(layers)\n",
        "#     parameters.append(activation)\n",
        "#     parameters.append(optimizer)\n",
        "\n",
        "#     return(parameters)"
      ],
      "metadata": {
        "id": "HtzRr4XzkWOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_population(size):\n",
        "    parameters = hyperparameters()\n",
        "\n",
        "    population = []\n",
        "    i=0\n",
        "    while i < size:\n",
        "        individual = [random.choice(parameters[j]) for j in range(len(parameters))]\n",
        "        if individual not in population:\n",
        "            population.append(individual)\n",
        "            i+=1\n",
        "    return(population)"
      ],
      "metadata": {
        "id": "cEYYYoCRkciE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(parameters,initialWeights=None):\n",
        "    neurons_per_layer = parameters[0]\n",
        "    no_of_layers = parameters[1]\n",
        "    activation_function = parameters[2]\n",
        "\n",
        "    #Following is not used here\n",
        "    # optimization_method = parameters[3]\n",
        "    regularization_param = parameters[3]\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=2, input_dim=3072))\n",
        "\n",
        "    for _ in range(no_of_layers):\n",
        "        model.add(Dense(units=neurons_per_layer, activation=activation_function,kernel_regularizer = tf.keras.regularizers.L2(regularization_param)))\n",
        "\n",
        "    model.add(Dense(units = 10,  activation = 'softmax',kernel_regularizer = tf.keras.regularizers.L2(regularization_param)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return(model)\n",
        "\n",
        "def evaluate_model(individual,initialWeights=None):\n",
        "    model = train_model(individual,initialWeights)\n",
        "\n",
        "    #The last element in the individual should always be the optimizer\n",
        "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"Adam\", metrics=['accuracy'])\n",
        "    model.fit(x_train, y_training, batch_size = 64, epochs = 10)\n",
        "\n",
        "    # print(\"Training Accuracy:\", model.evaluate(x_train, y_train, verbose = 0)[1])\n",
        "\n",
        "    #Evaluate on evaluation data\n",
        "    # loss_score, accuracy_score = model.evaluate(x_eval, y_val_array, verbose = 0)\n",
        "    y_pred = model(x_eval,training = False)\n",
        "    loss_score = loss_object(y_pred,y_val_array)\n",
        "    return loss_score,model"
      ],
      "metadata": {
        "id": "lwUtWm4BkiSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generations = 30\n",
        "population_size = 30\n",
        "\n",
        "initial_population = generate_population(population_size)\n",
        "# popultion_without_lp = initial_population\n",
        "losses = []\n",
        "models = []\n",
        "# losses = [evaluate_model(individual) for individual in initial_population\n",
        "for individual in initial_population:\n",
        "  loss,model = evaluate_model(individual)\n",
        "  losses.append(loss)\n",
        "  models.append(model)\n",
        "# losses_new = np.zeros(len(losses))\n",
        "# models_new = [None]*(len(models))\n",
        "# for i in range(len(models)):\n",
        "#   initial_population[i][3],models_new[i],losses_new[i] = perform_fine_tuning(models[i],initial_population[i])"
      ],
      "metadata": {
        "id": "tAknpQFNknOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3050ff4-85af-47c3-d497-73ddb5e94e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 3s 3ms/step - loss: 2.2968 - accuracy: 0.1385\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2227 - accuracy: 0.1950\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1511 - accuracy: 0.2310\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1018 - accuracy: 0.2400\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0636 - accuracy: 0.2360\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0100 - accuracy: 0.2560\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9872 - accuracy: 0.2560\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9794 - accuracy: 0.2575\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9406 - accuracy: 0.2660\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9423 - accuracy: 0.2710\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.3419 - accuracy: 0.1050\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1981 - accuracy: 0.1590\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1378 - accuracy: 0.1825\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0888 - accuracy: 0.1970\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0650 - accuracy: 0.2135\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0560 - accuracy: 0.2220\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0725 - accuracy: 0.1970\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0530 - accuracy: 0.2020\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0302 - accuracy: 0.2225\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0115 - accuracy: 0.2355\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3328 - accuracy: 0.0845\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2725 - accuracy: 0.1275\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2402 - accuracy: 0.1720\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1760 - accuracy: 0.2125\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1092 - accuracy: 0.2360\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0593 - accuracy: 0.2465\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0086 - accuracy: 0.2425\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9645 - accuracy: 0.2590\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9447 - accuracy: 0.2695\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9437 - accuracy: 0.2615\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3245 - accuracy: 0.0955\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2693 - accuracy: 0.1220\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2081 - accuracy: 0.1450\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1636 - accuracy: 0.1775\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1033 - accuracy: 0.2215\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0589 - accuracy: 0.2370\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0290 - accuracy: 0.2390\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0061 - accuracy: 0.2420\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9829 - accuracy: 0.2470\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9682 - accuracy: 0.2500\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.2807 - accuracy: 0.1485\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1805 - accuracy: 0.1805\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1256 - accuracy: 0.1960\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0555 - accuracy: 0.2195\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0076 - accuracy: 0.2425\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0345 - accuracy: 0.2180\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9912 - accuracy: 0.2365\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9457 - accuracy: 0.2590\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9421 - accuracy: 0.2575\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9198 - accuracy: 0.2630\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.2966 - accuracy: 0.1200\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2855 - accuracy: 0.1465\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2641 - accuracy: 0.1740\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2548 - accuracy: 0.1745\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2433 - accuracy: 0.1800\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2296 - accuracy: 0.1855\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2226 - accuracy: 0.1825\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2100 - accuracy: 0.1850\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2034 - accuracy: 0.1795\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1929 - accuracy: 0.1825\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.3440 - accuracy: 0.0825\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3167 - accuracy: 0.1010\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3067 - accuracy: 0.1010\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3033 - accuracy: 0.1015\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3015 - accuracy: 0.1375\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3008 - accuracy: 0.1440\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3002 - accuracy: 0.1130\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2991 - accuracy: 0.1030\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2977 - accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2960 - accuracy: 0.1175\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3024 - accuracy: 0.1195\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2775 - accuracy: 0.1485\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2071 - accuracy: 0.1975\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1767 - accuracy: 0.2150\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1461 - accuracy: 0.2160\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1082 - accuracy: 0.2310\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0890 - accuracy: 0.2285\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0708 - accuracy: 0.2245\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0389 - accuracy: 0.2390\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0119 - accuracy: 0.2430\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3178 - accuracy: 0.0970\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2970 - accuracy: 0.1195\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2964 - accuracy: 0.1115\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2869 - accuracy: 0.1250\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2816 - accuracy: 0.1255\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2827 - accuracy: 0.1405\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2556 - accuracy: 0.1605\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2442 - accuracy: 0.1615\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2177 - accuracy: 0.1670\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1797 - accuracy: 0.1740\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2632 - accuracy: 0.1560\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1720 - accuracy: 0.1745\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1054 - accuracy: 0.1775\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0942 - accuracy: 0.1830\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0762 - accuracy: 0.2020\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0226 - accuracy: 0.2210\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0031 - accuracy: 0.2360\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9767 - accuracy: 0.2410\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9656 - accuracy: 0.2475\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9428 - accuracy: 0.2565\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.3285 - accuracy: 0.0935\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2796 - accuracy: 0.1100\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2130 - accuracy: 0.1385\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1503 - accuracy: 0.1620\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0979 - accuracy: 0.1900\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0549 - accuracy: 0.2135\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0249 - accuracy: 0.2485\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9999 - accuracy: 0.2370\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9596 - accuracy: 0.2590\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9337 - accuracy: 0.2725\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3219 - accuracy: 0.1225\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2648 - accuracy: 0.1380\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2419 - accuracy: 0.1445\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2250 - accuracy: 0.1465\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2011 - accuracy: 0.1530\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1918 - accuracy: 0.1450\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1746 - accuracy: 0.1595\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1651 - accuracy: 0.1715\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1629 - accuracy: 0.1615\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1534 - accuracy: 0.1925\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.3069 - accuracy: 0.1120\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2984 - accuracy: 0.1245\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2900 - accuracy: 0.1250\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2691 - accuracy: 0.1665\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2453 - accuracy: 0.1590\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2359 - accuracy: 0.1670\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2071 - accuracy: 0.1860\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1910 - accuracy: 0.1815\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1662 - accuracy: 0.2005\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1562 - accuracy: 0.1930\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3335 - accuracy: 0.1145\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2339 - accuracy: 0.1845\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1735 - accuracy: 0.2055\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1201 - accuracy: 0.2265\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0819 - accuracy: 0.2130\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2.0363 - accuracy: 0.2195\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0019 - accuracy: 0.2400\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9749 - accuracy: 0.2510\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9619 - accuracy: 0.2500\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9552 - accuracy: 0.2570\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.3210 - accuracy: 0.0980\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3095 - accuracy: 0.0975\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3029 - accuracy: 0.1150\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2981 - accuracy: 0.1070\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2938 - accuracy: 0.1070\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2884 - accuracy: 0.1070\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2857 - accuracy: 0.1115\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2806 - accuracy: 0.1380\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2749 - accuracy: 0.1680\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2683 - accuracy: 0.1750\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.3558 - accuracy: 0.0915\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3201 - accuracy: 0.0940\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3055 - accuracy: 0.0925\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2964 - accuracy: 0.1025\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2906 - accuracy: 0.1165\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2878 - accuracy: 0.1295\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2814 - accuracy: 0.1685\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2793 - accuracy: 0.1690\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2754 - accuracy: 0.1695\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2724 - accuracy: 0.1720\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.2934 - accuracy: 0.1145\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2500 - accuracy: 0.1570\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2186 - accuracy: 0.1680\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1649 - accuracy: 0.1880\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1160 - accuracy: 0.1865\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0594 - accuracy: 0.2125\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0155 - accuracy: 0.2395\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0035 - accuracy: 0.2600\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0004 - accuracy: 0.2475\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9631 - accuracy: 0.2585\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.2897 - accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2274 - accuracy: 0.1530\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1753 - accuracy: 0.1810\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1200 - accuracy: 0.1985\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0720 - accuracy: 0.2075\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0575 - accuracy: 0.2135\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0410 - accuracy: 0.2115\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0257 - accuracy: 0.2185\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0080 - accuracy: 0.2260\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9814 - accuracy: 0.2320\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3399 - accuracy: 0.1060\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2776 - accuracy: 0.1165\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2331 - accuracy: 0.1420\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2020 - accuracy: 0.1665\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1626 - accuracy: 0.1845\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1428 - accuracy: 0.1835\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1180 - accuracy: 0.1900\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1012 - accuracy: 0.1910\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0764 - accuracy: 0.1995\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0827 - accuracy: 0.2125\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.3793 - accuracy: 0.1015\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3442 - accuracy: 0.1015\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3240 - accuracy: 0.1015\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3099 - accuracy: 0.1015\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2987 - accuracy: 0.1330\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2885 - accuracy: 0.1690\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2795 - accuracy: 0.1785\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2702 - accuracy: 0.1890\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2590 - accuracy: 0.1890\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2483 - accuracy: 0.1930\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.3148 - accuracy: 0.1080\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2843 - accuracy: 0.1385\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2476 - accuracy: 0.1535\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2174 - accuracy: 0.1570\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1744 - accuracy: 0.1620\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1435 - accuracy: 0.1700\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1176 - accuracy: 0.1675\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1380 - accuracy: 0.1610\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0977 - accuracy: 0.1900\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0687 - accuracy: 0.1940\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.3229 - accuracy: 0.1225\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2915 - accuracy: 0.1390\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2669 - accuracy: 0.1420\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2471 - accuracy: 0.1590\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2245 - accuracy: 0.1630\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1980 - accuracy: 0.1630\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1773 - accuracy: 0.1620\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1588 - accuracy: 0.1585\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1409 - accuracy: 0.1615\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1253 - accuracy: 0.1620\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3354 - accuracy: 0.0950\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3147 - accuracy: 0.1090\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2865 - accuracy: 0.1400\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2584 - accuracy: 0.1475\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2220 - accuracy: 0.1940\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1811 - accuracy: 0.1995\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1348 - accuracy: 0.2155\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1174 - accuracy: 0.2160\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0595 - accuracy: 0.2270\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0473 - accuracy: 0.2210\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3052 - accuracy: 0.1160\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2403 - accuracy: 0.1525\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2105 - accuracy: 0.1460\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1632 - accuracy: 0.1745\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1413 - accuracy: 0.1720\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1126 - accuracy: 0.1865\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0843 - accuracy: 0.1965\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0645 - accuracy: 0.1975\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0481 - accuracy: 0.2030\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0440 - accuracy: 0.2055\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3585 - accuracy: 0.1015\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2865 - accuracy: 0.1335\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2365 - accuracy: 0.1770\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1972 - accuracy: 0.2100\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1647 - accuracy: 0.2265\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1334 - accuracy: 0.2395\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1112 - accuracy: 0.2260\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0856 - accuracy: 0.2515\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0627 - accuracy: 0.2535\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0426 - accuracy: 0.2635\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 3ms/step - loss: 2.3301 - accuracy: 0.1010\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3163 - accuracy: 0.1010\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3083 - accuracy: 0.1010\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3042 - accuracy: 0.1065\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3021 - accuracy: 0.1150\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3003 - accuracy: 0.1240\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2996 - accuracy: 0.1210\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2984 - accuracy: 0.1445\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2978 - accuracy: 0.1520\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2970 - accuracy: 0.1570\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4223 - accuracy: 0.1555\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3090 - accuracy: 0.1785\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2400 - accuracy: 0.1900\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1876 - accuracy: 0.1900\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1475 - accuracy: 0.1910\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1264 - accuracy: 0.2005\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1062 - accuracy: 0.2110\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0808 - accuracy: 0.2240\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0685 - accuracy: 0.2290\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0530 - accuracy: 0.2335\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2501 - accuracy: 0.1440\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1868 - accuracy: 0.1460\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1451 - accuracy: 0.1610\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1016 - accuracy: 0.1770\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0761 - accuracy: 0.1795\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0588 - accuracy: 0.1965\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0448 - accuracy: 0.1980\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0629 - accuracy: 0.1875\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0427 - accuracy: 0.2010\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0358 - accuracy: 0.2085\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2985 - accuracy: 0.1325\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2342 - accuracy: 0.1650\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1595 - accuracy: 0.1875\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1298 - accuracy: 0.1855\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0881 - accuracy: 0.2100\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0830 - accuracy: 0.2085\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0821 - accuracy: 0.1910\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0348 - accuracy: 0.2245\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0294 - accuracy: 0.2310\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9861 - accuracy: 0.2530\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 2ms/step - loss: 2.3071 - accuracy: 0.1185\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1647 - accuracy: 0.1665\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0798 - accuracy: 0.2035\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0246 - accuracy: 0.2290\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9931 - accuracy: 0.2445\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9881 - accuracy: 0.2455\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9672 - accuracy: 0.2445\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9313 - accuracy: 0.2685\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9042 - accuracy: 0.2750\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9023 - accuracy: 0.2875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_without_lp = np.array(losses).min()"
      ],
      "metadata": {
        "id": "PdWMFBFBkrL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# population_without_lp"
      ],
      "metadata": {
        "id": "JZJGb4EXGb-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_param_without_lp = popultion_without_lp[np.array(losses).argmin()]"
      ],
      "metadata": {
        "id": "6qjcwFEBgOpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_param_without_lp"
      ],
      "metadata": {
        "id": "KjboT7EoG78u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# layers = best_param_without_lp[0]\n",
        "# neurons = best_param_without_lp[1]\n",
        "# regs = best_param_without_lp[3]\n",
        "# acti = best_param_without_lp[2]"
      ],
      "metadata": {
        "id": "haA-lxZlgVdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_wlp = Sequential()\n",
        "# model_wlp.add(Dense(units = 2,input_dim = 3072 ))\n",
        "# for i in range(layers):\n",
        "#   model_wlp.add(Dense(units = neurons, activation = acti, kernel_regularizer = tf.keras.regularizers.L2(regs)))\n",
        "# model_wlp.add(Dense(units = 10,activation = \"softmax\", kernel_regularizer = tf.keras.regularizers.L2(regs) ))"
      ],
      "metadata": {
        "id": "ePK3PtmUhCEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_wlp.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = \"Adam\", metrics = [\"accuracy\"])\n",
        "# model_wlp.fit(x_train,y_training, epochs = 10,batch_size = 64)\n",
        "# _,accuracy_wlp = model_wlp.evaluate(x_eval,y_val_array, verbose = 0)\n",
        "# # y_pred = model(x_eval,training = False)\n",
        "# # final_loss = loss_object(y_pred,y_val_array).numpy()"
      ],
      "metadata": {
        "id": "xQyzWjvThbDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_loss = losses_new.min()"
      ],
      "metadata": {
        "id": "ubboJIL4kyC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy_wlp"
      ],
      "metadata": {
        "id": "zTjd4VsoHBZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_param = initial_population[np.array(losses).argmin()]"
      ],
      "metadata": {
        "id": "yRAY_Sqwk1GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_param"
      ],
      "metadata": {
        "id": "8lXE6Xw_HExQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = models[np.array(losses).argmin()]"
      ],
      "metadata": {
        "id": "kuy04Q3-UOLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_eval = best_model(x_eval,training = False)"
      ],
      "metadata": {
        "id": "O_tnG2nOUSCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = best_model(x_test,training = False)"
      ],
      "metadata": {
        "id": "1nLWuqWtUVu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,accuracy = best_model.evaluate(x_eval,y_val_array, verbose = 0)"
      ],
      "metadata": {
        "id": "k0uAggXxUaWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,accuracy_test = best_model.evaluate(x_test,y_testing,verbose = 0)"
      ],
      "metadata": {
        "id": "d1xMC04uUgEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object(y_pred_eval,y_val_array).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PCWE2DFUkvL",
        "outputId": "f564e2d8-aac6-4aab-837d-a36c006efeba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.314523294587664"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object(y_pred_test,y_testing).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAvCsavZUopT",
        "outputId": "50792c3d-0db3-43b8-fb5f-0f897b81149d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.289574241268372"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG9zx11zUso9",
        "outputId": "e1880f90-fde7-4709-a3a5-02bbc147e4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24400000274181366"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yyl-P3LdUtet",
        "outputId": "c172c5fe-e0da-4528-c0a9-9b5e403d70fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25600001215934753"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reg = best_param[3]\n",
        "# number_of_layers=  best_param[0]\n",
        "# neuron_per_layer = best_param[1]\n",
        "# activation_function = best_param[2]\n"
      ],
      "metadata": {
        "id": "Zib4l-6_k3mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential()\n",
        "# model.add(Dense(units = 2,input_dim = 3072 ))\n",
        "# for i in range(number_of_layers):\n",
        "#   model.add(Dense(units = neuron_per_layer, activation = activation_function, kernel_regularizer = tf.keras.regularizers.L2(reg)))\n",
        "# model.add(Dense(units = 10,activation = \"softmax\", kernel_regularizer = tf.keras.regularizers.L2(reg) ))"
      ],
      "metadata": {
        "id": "h53N4tWKk6D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = \"Adam\", metrics = [\"accuracy\"])\n",
        "# model.fit(x_train,y_training, epochs = 10,batch_size = 64)\n",
        "# _,accuracy = model.evaluate(x_eval,y_val_array, verbose = 0)\n",
        "# # y_pred = model(x_eval,training = False)\n",
        "# # final_loss = loss_object(y_pred,y_val_array).numpy()"
      ],
      "metadata": {
        "id": "RVI19r_ck8Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_model_specs = {\"best_accuracy_with_lp\":accuracy,\"best_loss_with_lp\":final_loss,\"best_loss_without_lp\":loss_without_lp,\"Best_accuracy_without_lp\":accuracy_wlp}"
      ],
      "metadata": {
        "id": "WtnjIUEzfZsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_model_specs"
      ],
      "metadata": {
        "id": "SAYkaiSnGzlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle"
      ],
      "metadata": {
        "id": "-qP4JHA4k_E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file_to_save = open(\"random_search_with_lp.txt\",\"wb\")\n",
        "# pickle.dump(final_model_specs,file_to_save)\n",
        "# file_to_save.close()"
      ],
      "metadata": {
        "id": "sJrNDY4LfRH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model = models_new[np.array(losses_new).argmin()]"
      ],
      "metadata": {
        "id": "B3NU3nFiG5OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _,accuracy = best_model.evaluate(x_eval,y_val_array,verbose = 0)"
      ],
      "metadata": {
        "id": "bCcd0pa58yrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hot3DTI79AE_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}