{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParitoshP702/Bilevel-Optimization/blob/main/Genetic_Algorithm(MNIST).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvoUu9zWuWao"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import scipy as spy\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22rtNV6pfUBP"
      },
      "outputs": [],
      "source": [
        "pip install gurobipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGFTRB0hfVmN"
      },
      "outputs": [],
      "source": [
        "import gurobipy as gp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB27mmxBfXSV"
      },
      "outputs": [],
      "source": [
        " params = {\n",
        "  \"WLSACCESSID\": '753e7886-7142-449d-8baa-d41ca78716ef',\n",
        "  \"WLSSECRET\": '880d2525-364b-41d0-ac23-6dcf7ad15312',\n",
        "  \"LICENSEID\": 914249,\n",
        "  }\n",
        "env = gp.Env(params=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTBIGXLVYtNE"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "# #Use ! for global command.\n",
        "# !pwd\n",
        "# #Use % for the current shell. Note that cd will not work with !.\n",
        "# %cd \"/content/drive/MyDrive/csv files/IIM_A/loanacceptance.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uZuVD6Ofg5h"
      },
      "outputs": [],
      "source": [
        "def complete_weight_array(model):\n",
        "  weights_list = []\n",
        "  for i in range(len(model.weights)):\n",
        "    weights_array = tf.make_ndarray(tf.make_tensor_proto(model.weights[i]))\n",
        "    if i%2 == 0:\n",
        "      shape_array = weights_array.shape\n",
        "      for j in range(shape_array[0]):\n",
        "        for k in range(shape_array[1]):\n",
        "          weights_list.append(weights_array[j][k])\n",
        "          # if len(weights_list_new) < skip_length:\n",
        "          #   weights_list_new.append(0)\n",
        "          # else:\n",
        "          #   weights_list_new.append(weights_array[j][k])\n",
        "    else:\n",
        "      lgt = weights_array.shape[0]\n",
        "      for j in range(lgt):\n",
        "        weights_list.append(weights_array[j])\n",
        "        # if len(weights_list_new) < skip_length:\n",
        "        #   weights_list_new.append(0)\n",
        "        # else:\n",
        "        #   weights_list_new.append(weights_array[j])\n",
        "  return np.array(weights_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgH3pjvCflpM"
      },
      "outputs": [],
      "source": [
        "def weight_array_for_hessian(model):\n",
        "  skip_length = len(model.layers[0].weights[0].numpy().reshape(-1)) + len(model.layers[0].weights[1].numpy().reshape(-1))\n",
        "  weights_list = []\n",
        "  for i in range(len(model.weights)):\n",
        "    weights_array = tf.make_ndarray(tf.make_tensor_proto(model.weights[i]))\n",
        "    if i%2 == 0:\n",
        "      shape_array = weights_array.shape\n",
        "      for j in range(shape_array[0]):\n",
        "        for k in range(shape_array[1]):\n",
        "          # weights_list.append(weights_array[j][k])\n",
        "          if len(weights_list) < skip_length:\n",
        "            weights_list.append(0)\n",
        "          else:\n",
        "            weights_list.append(weights_array[j][k])\n",
        "    else:\n",
        "      lgt = weights_array.shape[0]\n",
        "      for j in range(lgt):\n",
        "        # weights_list.append(weights_array[j])\n",
        "        if len(weights_list) < skip_length:\n",
        "          weights_list.append(0)\n",
        "        else:\n",
        "          weights_list.append(weights_array[j])\n",
        "  return np.array(weights_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYfH9xyDfn1O"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(x_target,y_target,model):##general function which returns the list of gradient vector as an numpy array\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_object = tf.keras.losses.MeanSquaredError()\n",
        "    y_pred_array = model(x_target,training = True)\n",
        "    loss = loss_object(y_target,y_pred_array)\n",
        "  g = tape.gradient(loss,model.trainable_variables)\n",
        "  final_grad_list = []\n",
        "  for i in range(len(g)):\n",
        "    grad_array = tf.make_ndarray(tf.make_tensor_proto(g[i]))\n",
        "    if i%2==0:\n",
        "      grad_shape = grad_array.shape\n",
        "      for j in range(grad_shape[0]):\n",
        "        for k in range(grad_shape[1]):\n",
        "          final_grad_list.append(grad_array[j][k])\n",
        "    else:\n",
        "      length = grad_array.shape[0]\n",
        "      for j in range(length):\n",
        "        final_grad_list.append(grad_array[j])\n",
        "  return np.array(final_grad_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwjXdSrifp6P"
      },
      "outputs": [],
      "source": [
        "def compute_hessian(model):\n",
        "  final_hessian_list = []\n",
        "  with tf.GradientTape(persistent = True) as tape1:\n",
        "    with tf.GradientTape(persistent = True) as tape2:\n",
        "      loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "      y_pred_array = model(x_train,training = True)\n",
        "      loss = loss_object(y_training,y_pred_array)\n",
        "    g = tape2.gradient(loss, model.trainable_variables)\n",
        "  for i in range(len(g)):\n",
        "    # reshaped_grad = tf.reshape(g[i], [-1])\n",
        "    h = tape1.jacobian(g[i],model.trainable_variables)\n",
        "    final_hessian_list.append(h)\n",
        "\n",
        "\n",
        "  ##Now this final hessian list is actually a double dimensional list of tensors, so we will convert it into a matrix\n",
        "  #reshaping the double dimensional list of tensors into a matrix\n",
        "  hessian_matrix = np.empty(shape = (1,1),dtype = float)\n",
        "  for i in range(len(final_hessian_list)):\n",
        "    hess_col_mat = np.empty(shape = (1,1),dtype = float)\n",
        "    for j in range(len(final_hessian_list[i])):\n",
        "      hess_array = tf.make_ndarray(tf.make_tensor_proto(final_hessian_list[i][j]))\n",
        "      hess_shape = hess_array.shape\n",
        "      if i%2 == 0:\n",
        "        if j%2 == 0:\n",
        "          hess_array = hess_array.reshape(hess_shape[0]*hess_shape[1],hess_shape[2]*hess_shape[3])\n",
        "        else:\n",
        "          hess_array = hess_array.reshape(hess_shape[0]*hess_shape[1],hess_shape[2])\n",
        "      else:\n",
        "        if j%2 == 0:\n",
        "          hess_array = hess_array.reshape(hess_shape[0],hess_shape[1]*hess_shape[2])\n",
        "        else:\n",
        "          hess_array = hess_array\n",
        "      if j==0 :\n",
        "        hess_col_mat = hess_array\n",
        "      else:\n",
        "        hess_col_mat = np.concatenate((hess_col_mat,hess_array),axis = 1)\n",
        "    if i==0:\n",
        "      hessian_matrix = hess_col_mat\n",
        "    else:\n",
        "      hessian_matrix= np.concatenate((hessian_matrix,hess_col_mat),axis = 0)\n",
        "\n",
        "\n",
        "  return hessian_matrix\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Rd22Y6efsbl"
      },
      "outputs": [],
      "source": [
        "def perform_fine_tuning(model,params_model):\n",
        "  number_of_layers = params_model[1]\n",
        "  reg_param = params_model[3]\n",
        "  neurons_per_layer = params_model[0]\n",
        "  activation_fun = params_model[2]\n",
        "\n",
        "  ###calculating the hessian for the model and the gradient of the validation loss\n",
        "  hessian_matrix = compute_hessian(model)\n",
        "  grad_validation = compute_gradient(x_eval,y_val_array,model)\n",
        "  final_weights_array_new = weight_array_for_hessian(model)\n",
        "  l = len(final_weights_array_new)\n",
        "\n",
        "\n",
        "  ##adding the regularization term in the hessian\n",
        "  weight_array_vec = final_weights_array_new.reshape(l,1)/len(y_train)\n",
        "  hessian_col_mat = np.concatenate((weight_array_vec,hessian_matrix),axis = 1)\n",
        "  weight_array_withreg = np.concatenate(([[0]],final_weights_array_new.reshape(1,l)),axis = 1)/len(y_train)\n",
        "  hessian_mat_with_reg = np.concatenate((weight_array_withreg,hessian_col_mat),axis = 0)\n",
        "\n",
        "\n",
        "  grad_validation_new = np.concatenate(([[0]],grad_validation.reshape(1,l)),axis = 1)#validation array with regularization\n",
        "\n",
        "\n",
        "  ##Solving the linear program\n",
        "  ub = [10 for i in range(l+1)]\n",
        "  lb = []\n",
        "  for i in range(l+1):\n",
        "    if i==0:\n",
        "      lb.append(1e-5)\n",
        "    else:\n",
        "      lb.append(-10)\n",
        "\n",
        "\n",
        "  # Create the model within the Gurobi environment\n",
        "  m = gp.Model(env=env)\n",
        "  # m = gp.Model()\n",
        "  x = m.addMVar((l+1,),lb = lb, ub = ub )\n",
        "  m.setObjective(grad_validation_new@x)\n",
        "  # m.addConstr(hessian_mat_with_reg@x == 0)\n",
        "  m.addConstr(hessian_mat_with_reg@x <= 0.1)\n",
        "  m.addConstr(hessian_mat_with_reg@x >= -0.1)\n",
        "  x.PStart = np.zeros(l+1)\n",
        "  # GRBModel.Set(Pstart = np.zeros(l+1))\n",
        "  m.optimize()\n",
        "  all_vars = m.getVars()\n",
        "  values = m.getAttr(\"x\",all_vars)\n",
        "  values = np.array(values)\n",
        "  values = values/np.linalg.norm(values)\n",
        "\n",
        "  final_weights_array = complete_weight_array(model)\n",
        "  weight_array_with_reg = np.concatenate(([[reg_param]],final_weights_array.reshape(1,l)),axis = 1).reshape(-1)\n",
        "  descent_factors = []\n",
        "  for i in range(-100,20000,200):\n",
        "    descent_factors.append(i*1e-3)\n",
        "  descent_factors = np.array(descent_factors)\n",
        "\n",
        "\n",
        "  weight_sample_space_matrix = np.empty(shape = (len(descent_factors),len(weight_array_with_reg)),dtype = float)##initializing the weight sample space matrix\n",
        "  for i in range(len(descent_factors)):\n",
        "    weight_sample_space_matrix[i] =weight_array_with_reg+ values*descent_factors[i]   ##assigning values to the weight sample space matrix\n",
        "\n",
        "\n",
        "  ##defining the loss object\n",
        "  loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "  ##computation for validation loss\n",
        "  def validation_loss_computation(weight_and_reg_array):##function which computes the loss score of the model corresponding to given weights\n",
        "\n",
        "      model_demo = Sequential()\n",
        "      model_demo.add(Dense(units = 3, input_dim = 784))\n",
        "      for i in range(number_of_layers):\n",
        "          model_demo.add(Dense(units = neurons_per_layer, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.L2(weight_and_reg_array[0])))\n",
        "      model_demo.add(Dense(units = 10,activation = \"softmax\", kernel_regularizer = tf.keras.regularizers.L2(weight_and_reg_array[0])))\n",
        "      model_demo.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = \"Adam\", metrics = [\"accuracy\"])\n",
        "      weight_tracker = 1##as \"weight_and_reg_array\" is a one dimensional array it keeps track of the indices of the array\n",
        "      for i in range(len(model_demo.layers)):##changing the weights of the model layer wise\n",
        "        orignal_weight_list = model.layers[i].weights\n",
        "        array_1 = orignal_weight_list[0].numpy()##array corresponding to the weight matrix\n",
        "        array_2 = orignal_weight_list[1].numpy()##array corresponding to the bias vector\n",
        "        array_1_new = weight_and_reg_array[weight_tracker:weight_tracker+array_1.shape[0]*array_1.shape[1]]\n",
        "        weight_tracker += array_1.shape[0]*array_1.shape[1]##updating the weight tracker\n",
        "        array_2_new = weight_and_reg_array[weight_tracker:weight_tracker + array_2.shape[0]]\n",
        "        weight_tracker += array_2.shape[0] #updating the weight tracker\n",
        "        array_1_new = array_1_new.reshape(array_1.shape) ##new weight matrix\n",
        "        array_2_new = array_2_new.reshape(array_2.shape) ##new bias vector\n",
        "        list_of_new_array = [] ##list of the new weight matrix and the new bias vector\n",
        "        list_of_new_array.append(array_1_new)\n",
        "        list_of_new_array.append(array_2_new)\n",
        "        model_demo.layers[i].set_weights(list_of_new_array) ##appending the new weights into the given layer of the model\n",
        "      y_pred_array = model_demo(np.array(x_eval),training = False)\n",
        "      y_pred_training = model_demo(np.array(x_train),training = False)\n",
        "      loss = loss_object(y_val_array,y_pred_array)\n",
        "      loss_t = loss_object(y_training,y_pred_training)\n",
        "      # loss1,_ = model_demo.evaluate(x_eval,y_eval,verbose= 0)\n",
        "      # loss2,_= model_demo.evaluate(x_train,y_train,verbose = 0)\n",
        "      return loss,loss_t,model_demo\n",
        "\n",
        "\n",
        "  loss_array_valid = np.empty(shape = len(descent_factors),dtype = float)##array to contain the training losses\n",
        "  loss_array_train = np.empty(shape = len(descent_factors),dtype = float)##array to contain the validation losses\n",
        "\n",
        "  for i in range(len(loss_array_valid)):\n",
        "    loss_array_valid[i] ,loss_array_train[i],_= validation_loss_computation(weight_sample_space_matrix[i])\n",
        "\n",
        "\n",
        "  ideal_weight_array = weight_sample_space_matrix[loss_array_valid.argmin()]\n",
        "  ideal_regularization_parameter = ideal_weight_array[0]\n",
        "  _,_,best_model = validation_loss_computation(ideal_weight_array)\n",
        "\n",
        "  return ideal_weight_array[0],best_model,loss_array_valid.min()\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ra8N2vBYvWs"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv034oX1-sgD"
      },
      "outputs": [],
      "source": [
        "# weight_array_for_hessian(models[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUMMzQNDf73Q"
      },
      "source": [
        "Loading and Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwudmyF-gA-S"
      },
      "outputs": [],
      "source": [
        "(X_train,Y_train) , (X_test,Y_test) = tf.keras.datasets.mnist.load_data(path = \"mnist.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPjJAU_KgIqP"
      },
      "outputs": [],
      "source": [
        "X_train = X_train/255.0\n",
        "X_test = X_test/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjrizMumgLVn"
      },
      "outputs": [],
      "source": [
        "train_count = 5000\n",
        "eval_count = 2500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79_4-KpXgNHn"
      },
      "outputs": [],
      "source": [
        "X_test = X_test[:eval_count,:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ngk5dmpwgPNg"
      },
      "outputs": [],
      "source": [
        "x_test = X_test.reshape(X_test.shape[0],-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZBT6T1AgRFw"
      },
      "outputs": [],
      "source": [
        "y_test = Y_test[:eval_count]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHPUcpMLgS35"
      },
      "outputs": [],
      "source": [
        "x_train = X_train[:train_count,:,:]\n",
        "x_eval = X_train[train_count:train_count+eval_count,:,:]\n",
        "y_train = Y_train[:train_count]\n",
        "y_eval = Y_train[train_count:train_count+eval_count]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43iAMqRdgVbC"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],-1)\n",
        "x_eval = x_eval.reshape(x_eval.shape[0],-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaVED151gXU7"
      },
      "outputs": [],
      "source": [
        "y_training = np.zeros(shape = (len(y_train),10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-AAHX_GgZ5y"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_train)):\n",
        "  # y_training[i][y_train_array[i]] = 1\n",
        "  for j in range(10):\n",
        "    if j != (y_train[i]):\n",
        "      y_training[i][j] = 0.0\n",
        "    else:\n",
        "      y_training[i][j] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZZxCnNRgcyf"
      },
      "outputs": [],
      "source": [
        "y_val_array = np.empty(shape = (len(y_eval),10),dtype = float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5AYEfOageue"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_eval)):\n",
        "   for j in range(10):\n",
        "    if j != (y_eval[i]):\n",
        "      y_val_array[i][j] = 0.0\n",
        "    else:\n",
        "      y_val_array[i][j] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1L4OyvzggfO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJLxJ6v44kpS"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LyCH_IRuk24"
      },
      "outputs": [],
      "source": [
        "def hyperparameters(returnAs='vals'):\n",
        "    parameters = {}\n",
        "\n",
        "    #Add other parameters here\n",
        "\n",
        "    parameters[\"neurons_per_layer\"] = [10,20,30,40,50]\n",
        "    parameters[\"number_of_layers\"] = [3,4,5]\n",
        "    parameters[\"activation_function\"] = ['relu', 'tanh', 'sigmoid']\n",
        "    parameters[\"regularization_parameter\"] = [1e-10,1e-9,1e-8,1e-7,1e-6]\n",
        "    #Search over regularization parameter as well\n",
        "    #parameters[\"regularization\"] = []\n",
        "\n",
        "    #Keep the last one as optimizer\n",
        "    #parameters[\"optimization_method\"] = ['adam', 'rmsprop']\n",
        "    # parameters[\"optimization_method\"] = ['adam']\n",
        "\n",
        "    if returnAs == 'dict': return(parameters)\n",
        "    if returnAs == 'vals': return(list(parameters.values()))\n",
        "    if returnAs == 'keys': return(list(parameters.keys()))\n",
        "\n",
        "# def hyperparameters_old():\n",
        "#     parameters = []\n",
        "#     units_per_layer = [5, 10, 15]\n",
        "#     layers = [1, 2, 3]\n",
        "#     activation = ['relu', 'tanh', 'sigmoid']\n",
        "#     optimizer = ['adam', 'rmsprop']\n",
        "#     parameters.append(units_per_layer)\n",
        "#     parameters.append(layers)\n",
        "#     parameters.append(activation)\n",
        "#     parameters.append(optimizer)\n",
        "\n",
        "#     return(parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMnaLwvKum-H"
      },
      "outputs": [],
      "source": [
        "hyperparameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6pVBCWLupaP"
      },
      "outputs": [],
      "source": [
        "hyperparameters('dict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT1aooYkurQi"
      },
      "outputs": [],
      "source": [
        "def generate_population(size):\n",
        "    parameters = hyperparameters()\n",
        "\n",
        "    population = []\n",
        "    i=0\n",
        "    while i < size:\n",
        "        individual = [random.choice(parameters[j]) for j in range(len(parameters))]\n",
        "        if individual not in population:\n",
        "            population.append(individual)\n",
        "            i+=1\n",
        "    return(population)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0HmeNohutQC"
      },
      "outputs": [],
      "source": [
        "generate_population(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhKw5RWwuwIi"
      },
      "outputs": [],
      "source": [
        "def new_child(parent1, parent2):\n",
        "    parent_size = len(parent1)\n",
        "    rint = random.randint(0, parent_size)\n",
        "    #child = [random.choice([parent1[i],parent2[i]]) for i in range(parent_size)]\n",
        "    child1 = parent1[:rint]+parent2[rint:]\n",
        "    child2 = parent2[:rint]+parent1[rint:]\n",
        "    child = random.choice([child1,child2])\n",
        "\n",
        "    return(child)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoA-4Xylu0rn"
      },
      "outputs": [],
      "source": [
        "parent1 = [15, 1, 'relu', 1e-10]\n",
        "parent2 = [10, 3, 'sigmoid', 1e-9]\n",
        "new_child(parent1,parent2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZVDzUJNu3nk"
      },
      "outputs": [],
      "source": [
        "def mutation(population):\n",
        "    parameters = hyperparameters()\n",
        "    for chromosome in population:\n",
        "        if random.random() < 0.1 :\n",
        "            key = random.choice(range(len(parameters)))\n",
        "            parameters = hyperparameters()\n",
        "            mutate_key = random.choice(parameters[key])\n",
        "            chromosome[key] = mutate_key\n",
        "\n",
        "    return(population)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT3x9QKO6N8t"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.CategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB2AIW8fu7gH"
      },
      "outputs": [],
      "source": [
        "def train_model(parameters,initialWeights=None):\n",
        "    neurons_per_layer = parameters[0]\n",
        "    no_of_layers = parameters[1]\n",
        "    # activation_function = parameters[2]\n",
        "\n",
        "    #Following is not used here\n",
        "    # optimization_method = parameters[3]\n",
        "    regularization_param = parameters[3]\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=3, input_dim=784))\n",
        "\n",
        "    for _ in range(no_of_layers):\n",
        "        model.add(Dense(units=neurons_per_layer, activation=\"relu\",kernel_regularizer = tf.keras.regularizers.L2(regularization_param)))\n",
        "\n",
        "    model.add(Dense(units = 10,  activation = 'softmax',kernel_regularizer = tf.keras.regularizers.L2(regularization_param)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return(model)\n",
        "\n",
        "def evaluate_model(individual,initialWeights=None):\n",
        "    model = train_model(individual,initialWeights)\n",
        "\n",
        "    #The last element in the individual should always be the optimizer\n",
        "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"Adam\", metrics=['accuracy'])\n",
        "    model.fit(x_train, y_training, batch_size = 64, epochs = 10)\n",
        "\n",
        "    # print(\"Training Accuracy:\", model.evaluate(x_train, y_train, verbose = 0)[1])\n",
        "\n",
        "    #Evaluate on evaluation data\n",
        "    # loss_score, accuracy_score = model.evaluate(x_eval, y_val_array, verbose = 0)\n",
        "    y_pred = model(x_eval,training = False)\n",
        "    loss_score = loss_object(y_pred,y_val_array)\n",
        "    return loss_score,model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "un7e7yY-u8QQ"
      },
      "outputs": [],
      "source": [
        "generations = 15\n",
        "population_size = 10\n",
        "\n",
        "initial_population = generate_population(population_size)\n",
        "losses = []\n",
        "models = []\n",
        "# losses = [evaluate_model(individual) for individual in initial_population]\n",
        "for individual in initial_population:\n",
        "  loss,model = evaluate_model(individual)\n",
        "  losses.append(loss)\n",
        "  models.append(model)\n",
        "for i in range(len(models)):\n",
        "  initial_population[i][3],models[i],losses[i] = perform_fine_tuning(models[i],initial_population[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IES2fyn16Hzm"
      },
      "outputs": [],
      "source": [
        "initial_population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RXfen0dz9zkk"
      },
      "outputs": [],
      "source": [
        "initial_population[0][3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kZBxfGXxu-GE"
      },
      "outputs": [],
      "source": [
        "print(initial_population)\n",
        "print(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HUihREMhYH7A"
      },
      "outputs": [],
      "source": [
        "generation_wise_best_loss = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv19Ua-6vAdQ"
      },
      "outputs": [],
      "source": [
        "#Append fitness to population\n",
        "population_with_fitness = [pop+[f] for pop,f in zip(initial_population,losses,models)]\n",
        "\n",
        "#Write the generation steps here\n",
        "for _ in tqdm(range(generations)):\n",
        "    #Includes start as well as end while generating random integers\n",
        "    r1 = random.randint(0, population_size-1)\n",
        "    r2 = random.randint(0, population_size-1)\n",
        "\n",
        "    parent1 = population_with_fitness[r1][0:-1]\n",
        "    parent2 = population_with_fitness[r2][0:-1]\n",
        "\n",
        "    number_of_offspring = 2\n",
        "    offspring = [new_child(parent1,parent2) for i in range(number_of_offspring)]\n",
        "    offspring = mutation(offspring)\n",
        "\n",
        "    offspring_losses = []\n",
        "    offspring_models = []\n",
        "\n",
        "    for individual in offspring:\n",
        "      loss,model = evaluate_model(individual)\n",
        "      offspring_losses.append(loss)\n",
        "      offspring_models.append(model)\n",
        "    for i in range(len(offspring_losses)):\n",
        "      offspring[i][3],offspring_models[i],offspring_losses[i] = perform_fine_tuning(offspring_models[i],offspring[i])\n",
        "    # offspring_fitness = [evaluate_model(individual) for individual in offspring]\n",
        "    # models.extend(offspring_models)\n",
        "    offspring_with_fitness = [pop+[f] for pop,f in zip(offspring,offspring_losses,offspring_models)]\n",
        "\n",
        "    population_with_fitness.extend(offspring_with_fitness)\n",
        "\n",
        "    #Sort in descending by fitness\n",
        "    population_with_fitness.sort(key = lambda i: i[-1])\n",
        "\n",
        "    #Keep the best members\n",
        "    population_with_fitness = population_with_fitness[0:population_size]\n",
        "    generation_wise_best_loss.append(offspring_with_fitness.sort(key = lambda i:i[-1])[0][4])\n",
        "\n",
        "best_individual = population_with_fitness[0][0:-1]\n",
        "best_fitness = population_with_fitness[0][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vtb_AzYDvCC4"
      },
      "outputs": [],
      "source": [
        "best_individual\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GRzXONqWvEwa"
      },
      "outputs": [],
      "source": [
        "best_fitness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka1f_WEIYaI_"
      },
      "outputs": [],
      "source": [
        "generation_wise_best_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu1lvSAdYcWC"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbDtm7pxYdhM"
      },
      "outputs": [],
      "source": [
        "file_to_store = open(\"GA_MNIST(with_lp).txt\",\"wb\")\n",
        "pickle.dump(generation_wise_best_loss, file_to_store)\n",
        "file_to_store.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = best_individual[5]"
      ],
      "metadata": {
        "id": "xfoiaNgprJUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_to_store = open(\"GA_MNIST_best_model.txt\",\"wb\")\n",
        "pickle.dump(best_model,file_to_store)\n",
        "file_to_store.close()"
      ],
      "metadata": {
        "id": "Vh9mGlWYsx34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRivaM77vGeA"
      },
      "outputs": [],
      "source": [
        "s = evaluate_model(best_individual)\n",
        "s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-lMfkLRvK7k"
      },
      "outputs": [],
      "source": [
        "population_with_fitness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1RVWytqvRsg"
      },
      "outputs": [],
      "source": [
        "best_individual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38n7kzebvTgS"
      },
      "outputs": [],
      "source": [
        "neurons_per_layer = best_individual[0]\n",
        "no_of_layers = best_individual[1]\n",
        "activation_function = \"relu\"\n",
        "# optimization_method = best_individual[3]\n",
        "regularization_param = best_individual[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXTSFoWJvVct"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(units=6, input_dim=784))\n",
        "\n",
        "for _ in range(no_of_layers):\n",
        "  model.add(Dense(units=neurons_per_layer, activation=activation_function, kernel_regularizer = tf.keras.regularizers.L2(regularization_param)))\n",
        "model.add(Dense(units = 10,  activation = 'softmax', kernel_regularizer = tf.keras.regularizers.L2(regularization_param)))\n",
        "\n",
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"Adam\", metrics=['accuracy'])\n",
        "model.fit(x_train, y_training, batch_size = 64, epochs = 10)\n",
        "\n",
        "# loss_score, accuracy_score = model.evaluate(x_eval,y_eval, verbose = 0)\n",
        "y_pred = model(x_eval,training = False)\n",
        "loss_score = loss_object(y_pred,y_val_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOG17-invXL3"
      },
      "outputs": [],
      "source": [
        "# accuracy_score##accuracy score for the best fit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6KSdFIJvdtT"
      },
      "outputs": [],
      "source": [
        "loss_score# model.trainable_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dF7EH_tM7GP"
      },
      "outputs": [],
      "source": [
        "y_pred_testing = model(x_test,training = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-W0Raq3UUIB"
      },
      "outputs": [],
      "source": [
        "y_test_array = np.zeros(shape = (len(y_test),10),dtype = float)##trying to one hot encode the array for finding the cross entropy loss\n",
        "for i in range(len(y_test)):\n",
        "  for j in range(10):\n",
        "    if j != (y_test[i]):\n",
        "      y_test_array[i][j] = 0.0\n",
        "    else:\n",
        "      y_test_array[i][j] = 1.0\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZs4lFBIUV2h"
      },
      "outputs": [],
      "source": [
        "loss_object(y_test_array,y_pred_testing)##we are finding the loss this way because we calculated the gradient and hessian from this kind of loss (and our entire analysis depends on this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vO1DTtsUYNt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "mount_file_id": "1hb_ZTXqA0hdZNgUs3oin5zx-9yaNXJL8",
      "authorship_tag": "ABX9TyMKMOskVIEi7r8ZAYFh0Cgt",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}